{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "6dpJQfUu7nJe"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CharRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
        "        super(CharRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers = n_layers\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, n_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, x, hidden):\n",
        "        #out, hidden = self.rnn(x)          # default\n",
        "        out, hidden = self.rnn(x, hidden)   # allows u resume from a state\n",
        "        out = self.fc(out)\n",
        "        return out, hidden\n",
        "\n",
        "    def nohistory_hidden_state(self, batch_size):\n",
        "        # return tensor of zeros as a begin\n",
        "        return torch.zeros(self.n_layers, batch_size, self.hidden_size)\n",
        "\n",
        "\n",
        "def generate(model, prefix_str, size, eof='$'):\n",
        "    model.eval()\n",
        "    chars = []\n",
        "\n",
        "    # Build the initial hidden from the given prefix\n",
        "    hidden = model.nohistory_hidden_state(1)\n",
        "    for char in prefix_str:\n",
        "        # Build a one-hot-encoding for the current char\n",
        "        char_tensor = torch.zeros(1, 1, vocab_size)\n",
        "        char_tensor[0, 0, char2int[char]] = 1\n",
        "        out, hidden = model(char_tensor, hidden)\n",
        "        #print(char, int2char[out.argmax().item()])\n",
        "\n",
        "    # Now hidden represents all input letters and its out can predict a letter\n",
        "\n",
        "    def generate_letter():\n",
        "        # given logits, compute the probabilities and sample a letter\n",
        "        p = torch.nn.functional.softmax(out[0, 0], dim=0).detach().numpy()\n",
        "\n",
        "        # select a random index (generation) based on the distribution (weights)\n",
        "        # This allows us to generate several possible answers, like chatgpt\n",
        "        char_idx = np.random.choice(vocab_size, p=p)\n",
        "        #char_idx = out.argmax().item() # this just select a single most probable answer\n",
        "\n",
        "        char = int2char[char_idx]\n",
        "        return char\n",
        "\n",
        "    for _ in range(size):\n",
        "        # use the last out logits to generate a new letter\n",
        "        char = generate_letter()\n",
        "        chars.append(char)\n",
        "\n",
        "        if char == eof:\n",
        "            break\n",
        "\n",
        "        char_tensor = torch.zeros(1, 1, vocab_size)\n",
        "        char_tensor[0, 0, char2int[char]] = 1\n",
        "        # Predict the next letter given the current one and its history\n",
        "        out, hidden = model(char_tensor, hidden)\n",
        "\n",
        "    return ''.join(chars)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "HzkE_qhp9tCv"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # lets use some letter as EOF like $\n",
        "    sequences = [\n",
        "        \"Get Skilled in Machine Learning$$$$\",\n",
        "        \"By CS-Get Skilled Academy$$$$\",\n",
        "        \"Instructor Mostafa Saad Ibrahim$$$$\"\n",
        "    ]\n",
        "    all_sequences_data = ''.join(sequences)\n",
        "    chars = tuple(set(all_sequences_data))\n",
        "    vocab_size = len(chars)\n",
        "\n",
        "    # Character to index and index to character mappings\n",
        "    char2int = {ch: ii for ii, ch in enumerate(chars)}\n",
        "    int2char = {ii: ch for ii, ch in enumerate(chars)}\n",
        "    eos = char2int['$']\n",
        "\n",
        "    # Prepare the model and optimizer\n",
        "    hidden_size = 128\n",
        "    n_layers = 1\n",
        "    batch_size = 1\n",
        "    n_epochs = 100\n",
        "    learning_rate = 0.01\n",
        "\n",
        "    model = CharRNN(vocab_size, hidden_size, vocab_size, n_layers)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Training the model\n",
        "    model.train()\n",
        "    for epoch in range(n_epochs):\n",
        "        for data in sequences:\n",
        "            # the whole document is a single sequence (one hidden state)\n",
        "\n",
        "            hidden = model.nohistory_hidden_state(batch_size)\n",
        "            # Below, I feed the whole sequence as a single batch (better performance for a few examples)\n",
        "            # We also can divide sequences to subsequences (but all on SAME hidden state): e.g. seq_length = 10\n",
        "            seq_length = len(data)  # explore values like 10 and 20\n",
        "            for batch in range(0, len(data) - seq_length + 1, seq_length):\n",
        "                X = torch.zeros(batch_size, seq_length, vocab_size)         # 1x35x30  (35 seq len, 30 voc size)\n",
        "                y = torch.zeros(batch_size, seq_length, dtype=torch.long)   # 1x30\n",
        "                s1, s2 = '', ''\n",
        "                for i in range(seq_length):\n",
        "                    s1, s2 = data[batch + i], data[batch + i + 1 if batch + i + 1 < len(data) else eos]\n",
        "                    X[0, i, char2int[s1]] = 1\n",
        "                    y[0, i] = char2int[s2]\n",
        "                    #print(s1, '*', s2)\n",
        "\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                output, hidden = model(X, hidden)\n",
        "                loss = criterion(output.squeeze(0), y.squeeze(0))\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                hidden.detach_()    # to avoid pytorch error for seq_length < len(data). comment and see\n",
        "\n",
        "                answers = torch.max(output.squeeze(0), dim=1)[1]\n",
        "                train_acc = torch.sum(answers == y.squeeze(0)) / y.squeeze(0).size()[0]\n",
        "\n",
        "\n",
        "            print(f'Epoch {epoch+1}, Loss: {loss.item():.3f} - Accuracy: {train_acc.item():.2f}')\n",
        "\n",
        "\n",
        "    # Generate some text that starts with this prefix\n",
        "    print(generate(model, size=50, prefix_str='Get Skilled'))\n",
        "    print(generate(model, size=50, prefix_str='By'))\n",
        "    print(generate(model, size=50, prefix_str='Instructor'))\n",
        "    print(generate(model, size=50, prefix_str='lls'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SSf_H1SBKdz",
        "outputId": "b2994a5b-dd37-41e1-82bc-fa42ecb582bf"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 3.415 - Accuracy: 0.00\n",
            "Epoch 1, Loss: 3.250 - Accuracy: 0.24\n",
            "Epoch 1, Loss: 3.536 - Accuracy: 0.14\n",
            "Epoch 2, Loss: 2.889 - Accuracy: 0.29\n",
            "Epoch 2, Loss: 2.957 - Accuracy: 0.17\n",
            "Epoch 2, Loss: 3.072 - Accuracy: 0.11\n",
            "Epoch 3, Loss: 2.611 - Accuracy: 0.23\n",
            "Epoch 3, Loss: 2.602 - Accuracy: 0.28\n",
            "Epoch 3, Loss: 2.902 - Accuracy: 0.29\n",
            "Epoch 4, Loss: 2.185 - Accuracy: 0.43\n",
            "Epoch 4, Loss: 2.158 - Accuracy: 0.31\n",
            "Epoch 4, Loss: 2.686 - Accuracy: 0.26\n",
            "Epoch 5, Loss: 2.042 - Accuracy: 0.46\n",
            "Epoch 5, Loss: 3.082 - Accuracy: 0.24\n",
            "Epoch 5, Loss: 2.551 - Accuracy: 0.23\n",
            "Epoch 6, Loss: 2.291 - Accuracy: 0.37\n",
            "Epoch 6, Loss: 2.186 - Accuracy: 0.34\n",
            "Epoch 6, Loss: 2.219 - Accuracy: 0.34\n",
            "Epoch 7, Loss: 1.995 - Accuracy: 0.40\n",
            "Epoch 7, Loss: 1.718 - Accuracy: 0.59\n",
            "Epoch 7, Loss: 2.386 - Accuracy: 0.26\n",
            "Epoch 8, Loss: 1.843 - Accuracy: 0.46\n",
            "Epoch 8, Loss: 1.270 - Accuracy: 0.69\n",
            "Epoch 8, Loss: 2.093 - Accuracy: 0.31\n",
            "Epoch 9, Loss: 1.780 - Accuracy: 0.46\n",
            "Epoch 9, Loss: 1.192 - Accuracy: 0.69\n",
            "Epoch 9, Loss: 2.351 - Accuracy: 0.31\n",
            "Epoch 10, Loss: 1.667 - Accuracy: 0.60\n",
            "Epoch 10, Loss: 0.980 - Accuracy: 0.83\n",
            "Epoch 10, Loss: 2.018 - Accuracy: 0.43\n",
            "Epoch 11, Loss: 1.528 - Accuracy: 0.63\n",
            "Epoch 11, Loss: 0.812 - Accuracy: 0.90\n",
            "Epoch 11, Loss: 1.743 - Accuracy: 0.49\n",
            "Epoch 12, Loss: 1.232 - Accuracy: 0.66\n",
            "Epoch 12, Loss: 1.023 - Accuracy: 0.79\n",
            "Epoch 12, Loss: 1.398 - Accuracy: 0.57\n",
            "Epoch 13, Loss: 1.041 - Accuracy: 0.80\n",
            "Epoch 13, Loss: 0.691 - Accuracy: 0.86\n",
            "Epoch 13, Loss: 1.304 - Accuracy: 0.54\n",
            "Epoch 14, Loss: 0.809 - Accuracy: 0.77\n",
            "Epoch 14, Loss: 0.624 - Accuracy: 1.00\n",
            "Epoch 14, Loss: 0.978 - Accuracy: 0.77\n",
            "Epoch 15, Loss: 0.649 - Accuracy: 0.83\n",
            "Epoch 15, Loss: 0.500 - Accuracy: 0.97\n",
            "Epoch 15, Loss: 0.764 - Accuracy: 0.83\n",
            "Epoch 16, Loss: 0.480 - Accuracy: 1.00\n",
            "Epoch 16, Loss: 0.369 - Accuracy: 0.97\n",
            "Epoch 16, Loss: 0.556 - Accuracy: 0.89\n",
            "Epoch 17, Loss: 0.451 - Accuracy: 0.91\n",
            "Epoch 17, Loss: 0.241 - Accuracy: 1.00\n",
            "Epoch 17, Loss: 0.429 - Accuracy: 0.97\n",
            "Epoch 18, Loss: 0.365 - Accuracy: 0.97\n",
            "Epoch 18, Loss: 0.174 - Accuracy: 1.00\n",
            "Epoch 18, Loss: 0.339 - Accuracy: 0.94\n",
            "Epoch 19, Loss: 0.233 - Accuracy: 0.97\n",
            "Epoch 19, Loss: 0.162 - Accuracy: 1.00\n",
            "Epoch 19, Loss: 0.245 - Accuracy: 1.00\n",
            "Epoch 20, Loss: 0.140 - Accuracy: 1.00\n",
            "Epoch 20, Loss: 0.163 - Accuracy: 0.97\n",
            "Epoch 20, Loss: 0.175 - Accuracy: 1.00\n",
            "Epoch 21, Loss: 0.106 - Accuracy: 1.00\n",
            "Epoch 21, Loss: 0.129 - Accuracy: 0.97\n",
            "Epoch 21, Loss: 0.136 - Accuracy: 1.00\n",
            "Epoch 22, Loss: 0.099 - Accuracy: 0.97\n",
            "Epoch 22, Loss: 0.101 - Accuracy: 1.00\n",
            "Epoch 22, Loss: 0.106 - Accuracy: 1.00\n",
            "Epoch 23, Loss: 0.085 - Accuracy: 0.97\n",
            "Epoch 23, Loss: 0.083 - Accuracy: 1.00\n",
            "Epoch 23, Loss: 0.084 - Accuracy: 1.00\n",
            "Epoch 24, Loss: 0.071 - Accuracy: 0.97\n",
            "Epoch 24, Loss: 0.076 - Accuracy: 0.97\n",
            "Epoch 24, Loss: 0.068 - Accuracy: 1.00\n",
            "Epoch 25, Loss: 0.063 - Accuracy: 0.97\n",
            "Epoch 25, Loss: 0.064 - Accuracy: 1.00\n",
            "Epoch 25, Loss: 0.056 - Accuracy: 1.00\n",
            "Epoch 26, Loss: 0.060 - Accuracy: 0.97\n",
            "Epoch 26, Loss: 0.054 - Accuracy: 1.00\n",
            "Epoch 26, Loss: 0.047 - Accuracy: 1.00\n",
            "Epoch 27, Loss: 0.054 - Accuracy: 0.97\n",
            "Epoch 27, Loss: 0.051 - Accuracy: 1.00\n",
            "Epoch 27, Loss: 0.041 - Accuracy: 1.00\n",
            "Epoch 28, Loss: 0.045 - Accuracy: 1.00\n",
            "Epoch 28, Loss: 0.049 - Accuracy: 1.00\n",
            "Epoch 28, Loss: 0.036 - Accuracy: 1.00\n",
            "Epoch 29, Loss: 0.041 - Accuracy: 1.00\n",
            "Epoch 29, Loss: 0.041 - Accuracy: 1.00\n",
            "Epoch 29, Loss: 0.032 - Accuracy: 1.00\n",
            "Epoch 30, Loss: 0.038 - Accuracy: 1.00\n",
            "Epoch 30, Loss: 0.036 - Accuracy: 1.00\n",
            "Epoch 30, Loss: 0.028 - Accuracy: 1.00\n",
            "Epoch 31, Loss: 0.029 - Accuracy: 1.00\n",
            "Epoch 31, Loss: 0.033 - Accuracy: 1.00\n",
            "Epoch 31, Loss: 0.026 - Accuracy: 1.00\n",
            "Epoch 32, Loss: 0.026 - Accuracy: 1.00\n",
            "Epoch 32, Loss: 0.028 - Accuracy: 1.00\n",
            "Epoch 32, Loss: 0.024 - Accuracy: 1.00\n",
            "Epoch 33, Loss: 0.022 - Accuracy: 1.00\n",
            "Epoch 33, Loss: 0.025 - Accuracy: 1.00\n",
            "Epoch 33, Loss: 0.022 - Accuracy: 1.00\n",
            "Epoch 34, Loss: 0.020 - Accuracy: 1.00\n",
            "Epoch 34, Loss: 0.021 - Accuracy: 1.00\n",
            "Epoch 34, Loss: 0.021 - Accuracy: 1.00\n",
            "Epoch 35, Loss: 0.019 - Accuracy: 1.00\n",
            "Epoch 35, Loss: 0.019 - Accuracy: 1.00\n",
            "Epoch 35, Loss: 0.019 - Accuracy: 1.00\n",
            "Epoch 36, Loss: 0.017 - Accuracy: 1.00\n",
            "Epoch 36, Loss: 0.017 - Accuracy: 1.00\n",
            "Epoch 36, Loss: 0.018 - Accuracy: 1.00\n",
            "Epoch 37, Loss: 0.016 - Accuracy: 1.00\n",
            "Epoch 37, Loss: 0.015 - Accuracy: 1.00\n",
            "Epoch 37, Loss: 0.017 - Accuracy: 1.00\n",
            "Epoch 38, Loss: 0.015 - Accuracy: 1.00\n",
            "Epoch 38, Loss: 0.014 - Accuracy: 1.00\n",
            "Epoch 38, Loss: 0.016 - Accuracy: 1.00\n",
            "Epoch 39, Loss: 0.014 - Accuracy: 1.00\n",
            "Epoch 39, Loss: 0.013 - Accuracy: 1.00\n",
            "Epoch 39, Loss: 0.016 - Accuracy: 1.00\n",
            "Epoch 40, Loss: 0.013 - Accuracy: 1.00\n",
            "Epoch 40, Loss: 0.012 - Accuracy: 1.00\n",
            "Epoch 40, Loss: 0.015 - Accuracy: 1.00\n",
            "Epoch 41, Loss: 0.012 - Accuracy: 1.00\n",
            "Epoch 41, Loss: 0.012 - Accuracy: 1.00\n",
            "Epoch 41, Loss: 0.014 - Accuracy: 1.00\n",
            "Epoch 42, Loss: 0.011 - Accuracy: 1.00\n",
            "Epoch 42, Loss: 0.011 - Accuracy: 1.00\n",
            "Epoch 42, Loss: 0.013 - Accuracy: 1.00\n",
            "Epoch 43, Loss: 0.011 - Accuracy: 1.00\n",
            "Epoch 43, Loss: 0.010 - Accuracy: 1.00\n",
            "Epoch 43, Loss: 0.013 - Accuracy: 1.00\n",
            "Epoch 44, Loss: 0.010 - Accuracy: 1.00\n",
            "Epoch 44, Loss: 0.010 - Accuracy: 1.00\n",
            "Epoch 44, Loss: 0.012 - Accuracy: 1.00\n",
            "Epoch 45, Loss: 0.010 - Accuracy: 1.00\n",
            "Epoch 45, Loss: 0.009 - Accuracy: 1.00\n",
            "Epoch 45, Loss: 0.012 - Accuracy: 1.00\n",
            "Epoch 46, Loss: 0.009 - Accuracy: 1.00\n",
            "Epoch 46, Loss: 0.009 - Accuracy: 1.00\n",
            "Epoch 46, Loss: 0.011 - Accuracy: 1.00\n",
            "Epoch 47, Loss: 0.009 - Accuracy: 1.00\n",
            "Epoch 47, Loss: 0.009 - Accuracy: 1.00\n",
            "Epoch 47, Loss: 0.011 - Accuracy: 1.00\n",
            "Epoch 48, Loss: 0.008 - Accuracy: 1.00\n",
            "Epoch 48, Loss: 0.008 - Accuracy: 1.00\n",
            "Epoch 48, Loss: 0.010 - Accuracy: 1.00\n",
            "Epoch 49, Loss: 0.008 - Accuracy: 1.00\n",
            "Epoch 49, Loss: 0.008 - Accuracy: 1.00\n",
            "Epoch 49, Loss: 0.010 - Accuracy: 1.00\n",
            "Epoch 50, Loss: 0.008 - Accuracy: 1.00\n",
            "Epoch 50, Loss: 0.008 - Accuracy: 1.00\n",
            "Epoch 50, Loss: 0.009 - Accuracy: 1.00\n",
            "Epoch 51, Loss: 0.008 - Accuracy: 1.00\n",
            "Epoch 51, Loss: 0.007 - Accuracy: 1.00\n",
            "Epoch 51, Loss: 0.009 - Accuracy: 1.00\n",
            "Epoch 52, Loss: 0.007 - Accuracy: 1.00\n",
            "Epoch 52, Loss: 0.007 - Accuracy: 1.00\n",
            "Epoch 52, Loss: 0.009 - Accuracy: 1.00\n",
            "Epoch 53, Loss: 0.007 - Accuracy: 1.00\n",
            "Epoch 53, Loss: 0.007 - Accuracy: 1.00\n",
            "Epoch 53, Loss: 0.009 - Accuracy: 1.00\n",
            "Epoch 54, Loss: 0.007 - Accuracy: 1.00\n",
            "Epoch 54, Loss: 0.007 - Accuracy: 1.00\n",
            "Epoch 54, Loss: 0.008 - Accuracy: 1.00\n",
            "Epoch 55, Loss: 0.007 - Accuracy: 1.00\n",
            "Epoch 55, Loss: 0.006 - Accuracy: 1.00\n",
            "Epoch 55, Loss: 0.008 - Accuracy: 1.00\n",
            "Epoch 56, Loss: 0.006 - Accuracy: 1.00\n",
            "Epoch 56, Loss: 0.006 - Accuracy: 1.00\n",
            "Epoch 56, Loss: 0.008 - Accuracy: 1.00\n",
            "Epoch 57, Loss: 0.006 - Accuracy: 1.00\n",
            "Epoch 57, Loss: 0.006 - Accuracy: 1.00\n",
            "Epoch 57, Loss: 0.007 - Accuracy: 1.00\n",
            "Epoch 58, Loss: 0.006 - Accuracy: 1.00\n",
            "Epoch 58, Loss: 0.006 - Accuracy: 1.00\n",
            "Epoch 58, Loss: 0.007 - Accuracy: 1.00\n",
            "Epoch 59, Loss: 0.006 - Accuracy: 1.00\n",
            "Epoch 59, Loss: 0.006 - Accuracy: 1.00\n",
            "Epoch 59, Loss: 0.007 - Accuracy: 1.00\n",
            "Epoch 60, Loss: 0.006 - Accuracy: 1.00\n",
            "Epoch 60, Loss: 0.005 - Accuracy: 1.00\n",
            "Epoch 60, Loss: 0.007 - Accuracy: 1.00\n",
            "Epoch 61, Loss: 0.006 - Accuracy: 1.00\n",
            "Epoch 61, Loss: 0.005 - Accuracy: 1.00\n",
            "Epoch 61, Loss: 0.007 - Accuracy: 1.00\n",
            "Epoch 62, Loss: 0.005 - Accuracy: 1.00\n",
            "Epoch 62, Loss: 0.005 - Accuracy: 1.00\n",
            "Epoch 62, Loss: 0.006 - Accuracy: 1.00\n",
            "Epoch 63, Loss: 0.005 - Accuracy: 1.00\n",
            "Epoch 63, Loss: 0.005 - Accuracy: 1.00\n",
            "Epoch 63, Loss: 0.006 - Accuracy: 1.00\n",
            "Epoch 64, Loss: 0.005 - Accuracy: 1.00\n",
            "Epoch 64, Loss: 0.005 - Accuracy: 1.00\n",
            "Epoch 64, Loss: 0.006 - Accuracy: 1.00\n",
            "Epoch 65, Loss: 0.005 - Accuracy: 1.00\n",
            "Epoch 65, Loss: 0.005 - Accuracy: 1.00\n",
            "Epoch 65, Loss: 0.006 - Accuracy: 1.00\n",
            "Epoch 66, Loss: 0.005 - Accuracy: 1.00\n",
            "Epoch 66, Loss: 0.005 - Accuracy: 1.00\n",
            "Epoch 66, Loss: 0.006 - Accuracy: 1.00\n",
            "Epoch 67, Loss: 0.005 - Accuracy: 1.00\n",
            "Epoch 67, Loss: 0.004 - Accuracy: 1.00\n",
            "Epoch 67, Loss: 0.006 - Accuracy: 1.00\n",
            "Epoch 68, Loss: 0.005 - Accuracy: 1.00\n",
            "Epoch 68, Loss: 0.004 - Accuracy: 1.00\n",
            "Epoch 68, Loss: 0.006 - Accuracy: 1.00\n",
            "Epoch 69, Loss: 0.004 - Accuracy: 1.00\n",
            "Epoch 69, Loss: 0.004 - Accuracy: 1.00\n",
            "Epoch 69, Loss: 0.005 - Accuracy: 1.00\n",
            "Epoch 70, Loss: 0.004 - Accuracy: 1.00\n",
            "Epoch 70, Loss: 0.004 - Accuracy: 1.00\n",
            "Epoch 70, Loss: 0.005 - Accuracy: 1.00\n",
            "Epoch 71, Loss: 0.004 - Accuracy: 1.00\n",
            "Epoch 71, Loss: 0.004 - Accuracy: 1.00\n",
            "Epoch 71, Loss: 0.005 - Accuracy: 1.00\n",
            "Epoch 72, Loss: 0.004 - Accuracy: 1.00\n",
            "Epoch 72, Loss: 0.004 - Accuracy: 1.00\n",
            "Epoch 72, Loss: 0.005 - Accuracy: 1.00\n",
            "Epoch 73, Loss: 0.004 - Accuracy: 1.00\n",
            "Epoch 73, Loss: 0.004 - Accuracy: 1.00\n",
            "Epoch 73, Loss: 0.005 - Accuracy: 1.00\n",
            "Epoch 74, Loss: 0.004 - Accuracy: 1.00\n",
            "Epoch 74, Loss: 0.004 - Accuracy: 1.00\n",
            "Epoch 74, Loss: 0.005 - Accuracy: 1.00\n",
            "Epoch 75, Loss: 0.004 - Accuracy: 1.00\n",
            "Epoch 75, Loss: 0.004 - Accuracy: 1.00\n",
            "Epoch 75, Loss: 0.005 - Accuracy: 1.00\n",
            "Epoch 76, Loss: 0.004 - Accuracy: 1.00\n",
            "Epoch 76, Loss: 0.004 - Accuracy: 1.00\n",
            "Epoch 76, Loss: 0.005 - Accuracy: 1.00\n",
            "Epoch 77, Loss: 0.004 - Accuracy: 1.00\n",
            "Epoch 77, Loss: 0.003 - Accuracy: 1.00\n",
            "Epoch 77, Loss: 0.004 - Accuracy: 1.00\n",
            "Epoch 78, Loss: 0.004 - Accuracy: 1.00\n",
            "Epoch 78, Loss: 0.003 - Accuracy: 1.00\n",
            "Epoch 78, Loss: 0.004 - Accuracy: 1.00\n",
            "Epoch 79, Loss: 0.004 - Accuracy: 1.00\n",
            "Epoch 79, Loss: 0.003 - Accuracy: 1.00\n",
            "Epoch 79, Loss: 0.004 - Accuracy: 1.00\n",
            "Epoch 80, Loss: 0.003 - Accuracy: 1.00\n",
            "Epoch 80, Loss: 0.003 - Accuracy: 1.00\n",
            "Epoch 80, Loss: 0.004 - Accuracy: 1.00\n",
            "Epoch 81, Loss: 0.003 - Accuracy: 1.00\n",
            "Epoch 81, Loss: 0.003 - Accuracy: 1.00\n",
            "Epoch 81, Loss: 0.004 - Accuracy: 1.00\n",
            "Epoch 82, Loss: 0.003 - Accuracy: 1.00\n",
            "Epoch 82, Loss: 0.003 - Accuracy: 1.00\n",
            "Epoch 82, Loss: 0.004 - Accuracy: 1.00\n",
            "Epoch 83, Loss: 0.003 - Accuracy: 1.00\n",
            "Epoch 83, Loss: 0.003 - Accuracy: 1.00\n",
            "Epoch 83, Loss: 0.004 - Accuracy: 1.00\n",
            "Epoch 84, Loss: 0.003 - Accuracy: 1.00\n",
            "Epoch 84, Loss: 0.003 - Accuracy: 1.00\n",
            "Epoch 84, Loss: 0.004 - Accuracy: 1.00\n",
            "Epoch 85, Loss: 0.003 - Accuracy: 1.00\n",
            "Epoch 85, Loss: 0.003 - Accuracy: 1.00\n",
            "Epoch 85, Loss: 0.004 - Accuracy: 1.00\n",
            "Epoch 86, Loss: 0.003 - Accuracy: 1.00\n",
            "Epoch 86, Loss: 0.003 - Accuracy: 1.00\n",
            "Epoch 86, Loss: 0.004 - Accuracy: 1.00\n",
            "Epoch 87, Loss: 0.003 - Accuracy: 1.00\n",
            "Epoch 87, Loss: 0.003 - Accuracy: 1.00\n",
            "Epoch 87, Loss: 0.004 - Accuracy: 1.00\n",
            "Epoch 88, Loss: 0.003 - Accuracy: 1.00\n",
            "Epoch 88, Loss: 0.003 - Accuracy: 1.00\n",
            "Epoch 88, Loss: 0.004 - Accuracy: 1.00\n",
            "Epoch 89, Loss: 0.003 - Accuracy: 1.00\n",
            "Epoch 89, Loss: 0.003 - Accuracy: 1.00\n",
            "Epoch 89, Loss: 0.004 - Accuracy: 1.00\n",
            "Epoch 90, Loss: 0.003 - Accuracy: 1.00\n",
            "Epoch 90, Loss: 0.003 - Accuracy: 1.00\n",
            "Epoch 90, Loss: 0.003 - Accuracy: 1.00\n",
            "Epoch 91, Loss: 0.003 - Accuracy: 1.00\n",
            "Epoch 91, Loss: 0.003 - Accuracy: 1.00\n",
            "Epoch 91, Loss: 0.003 - Accuracy: 1.00\n",
            "Epoch 92, Loss: 0.003 - Accuracy: 1.00\n",
            "Epoch 92, Loss: 0.003 - Accuracy: 1.00\n",
            "Epoch 92, Loss: 0.003 - Accuracy: 1.00\n",
            "Epoch 93, Loss: 0.003 - Accuracy: 1.00\n",
            "Epoch 93, Loss: 0.002 - Accuracy: 1.00\n",
            "Epoch 93, Loss: 0.003 - Accuracy: 1.00\n",
            "Epoch 94, Loss: 0.003 - Accuracy: 1.00\n",
            "Epoch 94, Loss: 0.002 - Accuracy: 1.00\n",
            "Epoch 94, Loss: 0.003 - Accuracy: 1.00\n",
            "Epoch 95, Loss: 0.003 - Accuracy: 1.00\n",
            "Epoch 95, Loss: 0.002 - Accuracy: 1.00\n",
            "Epoch 95, Loss: 0.003 - Accuracy: 1.00\n",
            "Epoch 96, Loss: 0.003 - Accuracy: 1.00\n",
            "Epoch 96, Loss: 0.002 - Accuracy: 1.00\n",
            "Epoch 96, Loss: 0.003 - Accuracy: 1.00\n",
            "Epoch 97, Loss: 0.003 - Accuracy: 1.00\n",
            "Epoch 97, Loss: 0.002 - Accuracy: 1.00\n",
            "Epoch 97, Loss: 0.003 - Accuracy: 1.00\n",
            "Epoch 98, Loss: 0.002 - Accuracy: 1.00\n",
            "Epoch 98, Loss: 0.002 - Accuracy: 1.00\n",
            "Epoch 98, Loss: 0.003 - Accuracy: 1.00\n",
            "Epoch 99, Loss: 0.002 - Accuracy: 1.00\n",
            "Epoch 99, Loss: 0.002 - Accuracy: 1.00\n",
            "Epoch 99, Loss: 0.003 - Accuracy: 1.00\n",
            "Epoch 100, Loss: 0.002 - Accuracy: 1.00\n",
            "Epoch 100, Loss: 0.002 - Accuracy: 1.00\n",
            "Epoch 100, Loss: 0.003 - Accuracy: 1.00\n",
            " in Machine Learning$\n",
            " CS-Get Skilled Academy$\n",
            " Mostafa Saad Ibrahim$\n",
            "edcing$\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zkd0eGTOJ8NN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}